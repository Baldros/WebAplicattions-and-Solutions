{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0wKD2XR9JytCe67P73bCA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Apresentação:\n","\n","    O objetivo desse código é estudar um pouco sobre o whisper da openai\n","    uma solução de transcrição de speeach to text gratuito.\n","\n","https://openai.com/index/whisper/\n","\n"],"metadata":{"id":"F5vDuoWYpzBe"}},{"cell_type":"code","source":["# Instalações:\n","!pip install git+https://github.com/openai/whisper.git\n","!pip install yt-dlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TaRxjZL0p92s","executionInfo":{"status":"ok","timestamp":1716488341337,"user_tz":180,"elapsed":120800,"user":{"displayName":"André Amorim","userId":"04859568826717067647"}},"outputId":"e56d1b11-17bd-4351-9427-8b94838e47dd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-n5rqmw1c\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-n5rqmw1c\n","  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.4)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n","Collecting tiktoken (from openai-whisper==20231117)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.14.0)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper==20231117)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n","Building wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=0f1057d1bff852f2b3e440e53f6a56b06a5dde59fed7fc669f27e6bcb0833caf\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-118mb5lu/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n","Successfully built openai-whisper\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n","Collecting yt-dlp\n","  Downloading yt_dlp-2024.4.9-py3-none-any.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting brotli (from yt-dlp)\n","  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2024.2.2)\n","Collecting mutagen (from yt-dlp)\n","  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n","  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.31.0)\n","Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.0.7)\n","Collecting websockets>=12.0 (from yt-dlp)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.7)\n","Installing collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n","Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.20.0 websockets-12.0 yt-dlp-2024.4.9\n"]}]},{"cell_type":"code","source":["video_url = input('Coloque a url do vídeo:\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCr_kKNjqczE","executionInfo":{"status":"ok","timestamp":1716488349382,"user_tz":180,"elapsed":6739,"user":{"displayName":"André Amorim","userId":"04859568826717067647"}},"outputId":"f3cda6c3-149b-4415-e752-bc5a021e57af"},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":["Coloque a url do vídeo:\n","https://www.youtube.com/watch?v=ftl8cKnjeDM&list=PL5TJqBvpXQv4Aj3XZqo6ra_9nH68QQolv&ab_channel=Programa%C3%A7%C3%A3oDin%C3%A2mica\n"]}]},{"cell_type":"code","source":["!yt-dlp -x -f bestaudio --audio-format mp3 -o \"%(id)s.%(ext)s\" $video_url"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EsmrzGcPrHoU","executionInfo":{"status":"ok","timestamp":1716488414964,"user_tz":180,"elapsed":24338,"user":{"displayName":"André Amorim","userId":"04859568826717067647"}},"outputId":"402cfb6c-a724-4d97-aa6f-10f7b9d4455a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[youtube] Extracting URL: https://www.youtube.com/watch?v=ftl8cKnjeDM\n","[youtube] ftl8cKnjeDM: Downloading webpage\n","[youtube] ftl8cKnjeDM: Downloading ios player API JSON\n","[youtube] ftl8cKnjeDM: Downloading android player API JSON\n","[youtube] ftl8cKnjeDM: Downloading player bc657243\n","[youtube] ftl8cKnjeDM: Downloading m3u8 information\n","[info] ftl8cKnjeDM: Downloading 1 format(s): 251\n","[download] Destination: ftl8cKnjeDM.webm\n","\u001b[K[download] 100% of   10.71MiB in \u001b[1;37m00:00:02\u001b[0m at \u001b[0;32m3.94MiB/s\u001b[0m\n","[ExtractAudio] Destination: ftl8cKnjeDM.mp3\n","Deleting original file ftl8cKnjeDM.webm (pass -k to keep)\n"]}]},{"cell_type":"code","source":["import whisper"],"metadata":{"id":"LOeETkIErdnt","executionInfo":{"status":"ok","timestamp":1716488433091,"user_tz":180,"elapsed":5932,"user":{"displayName":"André Amorim","userId":"04859568826717067647"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Modelo tiny\n","model = whisper.load_model(\"tiny\")\n","result=model.transcribe('/content/ftl8cKnjeDM.mp3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDEKDchRrtQl","executionInfo":{"status":"ok","timestamp":1716488624313,"user_tz":180,"elapsed":133860,"user":{"displayName":"André Amorim","userId":"04859568826717067647"}},"outputId":"064624db-1388-4cab-87bf-c02be63ede51"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████| 72.1M/72.1M [00:02<00:00, 36.2MiB/s]\n","/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]}]},{"cell_type":"code","source":["result['text']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"id":"0tFC7-bFxhXN","executionInfo":{"status":"ok","timestamp":1716489959680,"user_tz":180,"elapsed":4,"user":{"displayName":"André Amorim","userId":"04859568826717067647"}},"outputId":"8650274e-aabb-4323-bab8-f90c12e01953"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' A inspiração para esse vídeo vai desse pouche do Glyermi Felite que passou por mim no link a gente e ele disse que ajudou uma amiga jornalista a transcrever um vídeo do YouTube e aí compartilhou o código para que outras pessoas pudessem estar da gente bizarro. Oi gente, eu sou a Kizia e esse é o canal programação de Nânica aqui você encontra vídeo sobre esse no programação, se emcededados, inteligência artificial discussões sobre técnicos e legências impactos na sociedade se você acompanha o nosso canal e se interessa por terigência artificial você provavelmente já no dia de discutindo as diferenças aplicações de inteligência artificial nos impactos, as implicações exécticas inclusive que elas têm. Essas discussões são muito levantes, a gente vai continuar fazendo elas aqui no canal mas essa linha tutorial e a naprática é para a gente pensar em aplicações e a gente vai começar hoje aplicando a inteligência artificial para fazer transcrição de vídeos no YouTube e para isso a gente vai utilizar o whisper que é um sistema de reconhecimento de fala da OpenAI. Aqui no site da OpenAI, eles explica o que o whisper é um sistema de reconhecimento automático de fala, treinado em 680,8,8 horas de dados supervisionados em diversos de homas então multilínguas e multitaréfa coletados da web e eles conseguiram com eles conseguiram com a sistema mostrar que o uso de um conjunto de dados grande e diversificado do web a maior robustes aos sutáxios, os ruídos de fundo e a linguagem técnica. Permitindo a transcrição e vários idiomas e a tradução desse idiomas para o inglês. Então olha aqui interessante, além de você conseguir pegar um vídeo no YouTube, ningo português ou em romano jogar no whisper e fazer a transcrição, você também consegue jogar esse vídeo e pedir a versão em dois dele porque ele é capaz de fazer transcrição e tradução. Se você tem interesse em se aprofundar nesse modelo, você pode dar um alhada nesse site e você pode deixar o link aqui na descrição, mas como eu falei, eu sou vídeo aqui sobre aplicação. A inspiração para esse vídeo vai desse pouche do Glyermy Felite que passou por mim no link de ingin e ele disse que ajudou uma amiga jornalista a transcriver um vídeo do YouTube e compartilhou o código para que outras pessoas pudessem estar dentro de realizar. De fato eu sei que essa é uma demanda muito grande jornalistas, que fazem entrevistas ou que de fato pegam vídeos no YouTube já prontos e precisando a transcrição, eu já dei aula na VIA de jornalismo e percebi isso na prática, meus alunos, e jornalista tinha uma essa demanda mesmo. Para conseguir executar a transcrição do vídeo, a gente vai usar dois pacotes, o Open and Iwisper que justamente para fazer a transcrição, a gente também vai usar esse outro aqui que a gente conseguir fazer o download do audio de um vídeo do YouTube. E aí sim passar esse audio para o sistema de conhecimento de fala. Importante de destacar que para utilizar o U-Sper por meio desse pacote ou o Penenhaio U-Sper a gente não precisa pagar, é gratuito para fazer essa transcrição de vídeo e isso é impossível e to nainda mais interessante. De a possibilidade de fazer isso, certo? Então vamos lá, a gente vai começar instalando os pacotes aqui no nosso ambiente do Google Colab para isso a gente utiliza esse esclamação e o comando PIP-Instal seguida do nome do pacote, se você já programa em Python, você provavelmente está bem familiarizado com isso. Depois de instalar esses pacotes, nosso próximo passo é escolher o vídeo que a gente quer fazer a transcrição e fazer um download do audio desse vídeo utilizando o pacote ali que a U-Sper ter traz deli P, né, a abbreviação dele. O vídeo que eu vou escolher é o vídeo mais recente do canal, em que o alice me explica o que são redes neurais e eu preciso apenas conseguir pegar o link desse vídeo aqui do meu computador para colocar dentro do meu código. E eu vou guardar essa oi-re-re-re-re do esse vídeo do YouTube, não é variável que se chama vídeo a R-Ele. Para as some passas, a gente fazia o download do audio desse vídeo utilizando esse comando das pacotes e de São T-D-L-P, então, é a forma, essa é a sintar-se com esse pacote, é utilizado e a gente precisa ter nas rodar esse comando. E veja aqui no final, a gente está passando para ele ao a re-re-re-re-re-do vídeo em si. Veja então que ele gerou o arquivo de audio relacionado ao vídeo aqui nessa seção de arquivos, então, se não ser o notebook, não tiver aparecendo, tem que licar nessa pastinha aqui e aparece aqui ao lado. Então, uma vez que a gente já tem o vídeo, agora sim, a gente consegue usar o Vista para fazer a transcrição do vídeo. Para fazer a transcrição de vídeo, a gente agora sim vai usar o pacote o Vista, porque a gente instala. E você vai ver que é bem simples, a gente vai precisar de dois comanos específicamente. Carregar o modelo e fazer a transcrição a partir do modelo que a gente carregou. Para carregar o modelo, a gente precisa especificar qual modelo, qual versão do modelo. E a gente tem algumas opções, que é, tá aí em base, small, medium, and large. Que são as opções que variam de acordo com a qualidade de a transcrição e com o tamanho do modelo e processamento dele. Então, a gente vai fazer aqui gradualmente testes, desde o tiny, depois a gente vai testar o small e o medium, para entender o resultado, né, a qualidade de resultados e a diferença se de fato ele vai melhorando. Embora demora um pouco mais para processar, se ele consegue melhorar, na qualidade de transcrições. Mas,icamente, é isso aqui. A gente usa o Vista para o ponto load model, passa o tipo do modelo, aqui eu vou usar o tiny primeiro para a gente testar o mais simples. A gente tem tiny, base, small, medium, and large. O large é o maior modelo que tem a maior qualidade. E o maior tempo de processamento. Inseguida, a gente tem que indicar para ele. O que a gente quer é que ele transcribe. E aí, a gente vai passar o nome do arquivo aqui, que a gente baixou. Então, eu vou copiar aqui, de provavelmente vai copiar o caminho inteiro do arquivo, mas a gente vai passar só o nome dele mesmo aqui. E a gente pode rodar o modelo. Se a gente imprimia que o resultado, na tela a gente vê que ele tem, que ele é um discionário que tem monte de coisa. E uns atributos desse discionário é o texto da transcrição. Então, para a gente acessar especificamente, o texto a gente usa, então, o resulte texte. Ele está traduindo como não, e itas, complicado, de hoje você vai aprender o que são, redeu no gális, mas tem uma ideia de como as redeu no gális funcionam. Isso era para ser redes neurais. A gente pode fazer um novo texto utilizando o modelo um pouco melhor, e ver se a qualidade de a transcrição vai melhorar. Então, eu vou copiar esse mesmo código aqui. E agora vou passar o small para ele. Vou sair do tiny, vou plau, o big, vou pros small. Eu vou chamar de model 2 para eu não confundir de resultado 2. O vídeo que a gente quer passar é o mesmo. Então, agora a gente roda de novo. E provavelmente vai demorar um pouco mais. Era aí, na verdade, aqui não vai funcionar, né? Eu tenho que botar o model 2 aqui também. Agora sim. Enquanto a gente espera rodar, eu já vou preparar aqui a nossa próxima rodada, que vai ser um modelo 3, utilizando a versão média do modelo. Modelo 3.000 de rodar, agora a gente pode ver o resultado 2. Ele saiu de redeu no gális para rede neurais. Ou seja, ele se aproximou um pouco mais, porque seria um as redes neurais, mas ele ainda não chegou no resultado que a gente gostaria, né? A gente queria que eles creves de vifato redes neurais. Mas você vê que ele consegue entender coisas aqui melhores. Então, aquele tente de interligência artificial como interligência especial, aquele já transformou a interligência artificial. Então, vamos ver se um modelo na versão média, um consegue chegar na qualidade que a gente gostaria de menos identificar palavras, as duas palavras, na principal esivíduo que são redes neurais. O nosso modelo 1.000 de determinado rodar, e agora a expectativa para a gente ver o resultado. Veja que essa introdução aqui do vídeo, está bem difícil para ele pegar que seria no e-a descomplicada de hoje. Ele está transcrevendo como no e-ares complicada. Mas, finalmente, ele entendeu que o vídeo sobre redes neurais. Então, entendeu não, ele transcreveu o vídeo sobre redes neurais, usando a palavra redes neurais. Então, aqui ele consegue agora dizer que o você vai aprender o que são redes neurais, vai ter uma ideia de como as redes neurais funcionam. E também de porque elas são tão poderosas. Porque nós utilizamos redes neurais para fazer aplicações. Ficou muito boa essa versão. Então, acho que agora a gente já tem uma versão incusível legal para a gente salvar um arquivo de texto. Acho que essa uma funcionalidade que faz sentido, eu colocar aqui no vídeo, porque se você tá fazendo esse processo de transcrição para utilizar esse texto para alguma coisa. Mas, muito sentido, que você quer atirar o texto aqui de dentro do notebook e levar para algum outro lugar, por meio de um arquivo. Para isso, a gente vai utilizar esse pequeno transcricódigo. Eu abre um novo arquivo com o nome e com o ideia do vídeo, e eu tamanho do modelo que eu te dizer e ponto tester. Aí, aqui, eu cri uma variável, que vai ser esse arquivo de texto de eu vou escrever. Então, a cada ponto, cada centença que aparece, termina com o ponto, eu pego a sentença, escrevo no arquivo e polulinha, lá no meu arquivo. E aí, a gente pode rodar esse transcricódigo. E o arquivo tester também vai aparecer aqui na lateral. Então, a gente consegue abrir ele aqui, dentro do ambiente do colébe, e agora, assim, de fato, ler a transcrição inteira, que é bem legal. E a gente também consegue baixar, eu vou baixar aqui para a gente poder abrir dentro do computador, esse transcrição. Ah, eu cabei de perceber que eu sou o veia versão errado, eu queria salvar a versão do modelo 3, mas eu sou o veia versão do modelo 1 aqui. Eu tenho que mudar aqui para o Vizote 3. E aí, assim, agora, provavelmente, está na versão correta. Agora, assim, então, vou baixar novamente. Então, veia que é uma transcrição bem longo, né? A liazes se vídeo um vídeo muito longo, um vídeo de 18 minutos. Ele não demorou tanto, né? A gente vê o quanto, as inteligências eficiais realmente avançaram, porque a gente agora tem acesso gratuito, a transcrição de um vídeo longo. E eu não vou fazer aqui a versão large, se você quiser testar em sua casa testa e me conta depois, se você conseguir também notar um ganho de melhor na qualidade de a transcrição, utilizando a maior versão do modelo, se você gostou desse vídeo dessa nova lia de tutorial e a anaprática aqui no canal, deixe um comentário utilizando a hashtag. E ah, esse vídeo fica por aqui, muito obrigado e até a próxima. A gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui, e a gente tem que fazer aqui,'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["model2 = whisper.load_model(\"small\")\n","result2=model2.transcribe('ftl8cKnjeDM.mp3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VwfkrKyxr8rV","executionInfo":{"status":"ok","timestamp":1716489186026,"user_tz":180,"elapsed":561063,"user":{"displayName":"André Amorim","userId":"04859568826717067647"}},"outputId":"e3180d36-651e-411b-dd5b-5cf35f2fa3b1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 461M/461M [00:11<00:00, 41.4MiB/s]\n","/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]}]},{"cell_type":"code","source":["result2['text']"],"metadata":{"id":"x59HD85uxk_E","executionInfo":{"status":"ok","timestamp":1716489970794,"user_tz":180,"elapsed":250,"user":{"displayName":"André Amorim","userId":"04859568826717067647"}},"outputId":"19f278b3-fb8d-4055-ac10-6589b780338a","colab":{"base_uri":"https://localhost:8080/","height":226}},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' A inspiração para esse vídeo veio desse post do Guilherme Felici, que passou por mim no LinkedIn, e ele disse que ajudou uma amiga jornalista a transcrever um vídeo do YouTube, e aí compartilhou o código para que outras pessoas pudessem também utilizar. Oi gente, eu sou a Kiz e esse é o canal Programação Dinâmica. Aqui você encontra vídeos sobre ensino de programação, ciência de dados, inteligência artificial, discussões sobre tecnologias e seus impactos na sociedade. Se você acompanha o nosso canal e se interessa por inteligência artificial, você provavelmente já nos viu discutindo as diferentes aplicações de inteligência artificial e os impactos, as implicações éticas inclusive que elas têm. Essas discussões são muito relevantes, a gente vai continuar fazendo elas aqui no canal, mas essa linha editorial e a Ana Prática é para a gente pensar em aplicações. E a gente vai começar hoje aplicando inteligência artificial para fazer transcrição de vídeos no YouTube, e para isso a gente vai utilizar o Whisper, que é um sistema de reconhecimento de fala da OpenAI. Aqui no site da OpenAI eles explicam que o Whisper é um sistema de reconhecimento automático de fala treinado em 680 mil horas de dados supervisionados em diversos idiomas, então multilinguês e multitarefa coletados da web. E eles conseguiram com esse sistema mostrar que o uso de um conjunto de dados grande e diversificado leva uma maior robustez aos sotaques, as ruídos de fundo e a linguagem técnica, permitindo a transcrição em vários idiomas e a tradução desses idiomas para o inglês. Então olha que interessante, além de você conseguir pegar um vídeo no YouTube em língua portuguesa ou em Romeno, jogar no Whisper e fazer a transcrição, você também consegue jogar esse vídeo e pedir a versão em inglês dele, porque ele é capaz de fazer transcrição e tradução. Se você tem interesse em se aprofundar nesse modelo, você pode dar uma olhada nesse site, eu vou deixar o link aqui na descrição, mas como eu falei, o nosso vídeo aqui é sobre a aplicação. A inspiração para esse vídeo veio desse post do Guilherme Felici, que passou por mim no LinkedIn, e ele disse que ajudou uma amiga jornalista a transcrever um vídeo do YouTube, e aí compartilhou o código para que outras pessoas pudessem também utilizar. De fato, eu sei que essa é uma demanda muito grande de jornalistas, que fazem entrevistas, ou que de fato pegam vídeos no YouTube já prontos e precisam da transcrição. Eu já dei aula no MBE de jornalismo e percebi isso na prática, os meus alunos jornalistas tinham essa demanda mesmo. Para conseguir executar a transcrição do vídeo, a gente vai usar dois pacotes, o OpenAI Whisper, que é justamente para fazer a transcrição, e a gente também vai usar esse outro aqui, que é para a gente conseguir fazer o download do áudio de um vídeo do YouTube. E aí sim, passar esse áudio para o sistema de reconhecimento de fala. É importante destacar que para utilizar o Whisper por meio desse pacote, OpenAI Whisper, a gente não precisa pagar, é gratuito para fazer essa transcrição de vídeo. Isso, inclusive, torna ainda mais interessante ter a possibilidade de fazer isso, certo? Então, vamos lá, a gente vai começar instalando os pacotes aqui no nosso ambiente do Google Collab, para isso a gente utiliza essa exclamação e o comando PIP INSTAL, seguida do nome do pacote. Se você já programou em Python, você provavelmente está bem familiarizado com isso. Depois de instalar esses pacotes, o nosso próximo passo é escolher o vídeo que a gente quer fazer, a transcrição, e fazer um download do áudio desse vídeo, utilizando o pacote ali, que é o YT, atrás da DLP, a abreviação dele. O vídeo que eu vou escolher é o vídeo mais recente do canal, em que o Arsene explica o que são redes neurais, e eu preciso apenas conseguir pegar o link desse vídeo aqui do meu computador, para colocar ele dentro do meu código. Eu vou guardar essa URL desse vídeo do YouTube, numa variável que se chama vídeo URL. O próximo passo é a gente fazer o download do áudio desse vídeo, utilizando esse comando desse pacote YT DLP. Então, é a forma, essa é a sintasse, como esse pacote é utilizado, e a gente precisa apenas rodar esse comando. E veja que aqui no final a gente está passando para ele a URL do vídeo em si. Então, ele gerou o arquivo de áudio relacionado ao vídeo, aqui nessa sessão de arquivos. Então, se no seu notebook não estiver aparecendo, você tem que clicar nessa pastinha aqui, e aparece aqui ao lado. Então, uma vez que a gente já tem o vídeo, agora sim, a gente consegue usar o whisper para fazer a transcrição do vídeo. Para fazer a transcrição de vídeo, a gente agora sim vai usar o pacote whisper que a gente instalou. E você vai ver que é bem simples, a gente vai precisar de dois comandos especificamente. A gente vai carregar o modelo e fazer a transcrição a partir do modelo que a gente carregou. Para carregar o modelo, a gente precisa especificar qual modelo, qual versão do modelo. E a gente tem algumas opções, que é tiny, base, small, medium e enlarge. Que são as opções que variam de acordo com a qualidade da transcrição e com o tamanho do modelo e o processamento dele. Então, a gente vai fazer aqui gradualmente testes, desde o tiny, depois a gente vai testar o small e o medium para entender o resultado, e as qualidades e resultados e a diferença. Se, de fato, ele vai melhorando, embora demore um pouco mais para processar, se ele consegue melhorar na qualidade das transcrições. Basicamente é isso aqui, a gente usa o whisper.loadModel, passa o tipo do modelo. Aqui eu vou usar o tiny primeiro, para a gente testar o mais simples. A gente tem tiny, base, small, medium e large. O large é o maior modelo que tem a maior qualidade e o maior tempo de processamento. Em seguida, a gente tem que indicar para ele o que a gente quer que ele transcreva. E aí a gente vai passar o nome do arquivo aqui que a gente baixou. Então, eu vou copiar aqui. Ele provavelmente vai copiar o caminho inteiro do arquivo, mas a gente vai passar só o nome dele mesmo aqui. E a gente pode rodar o modelo. Se a gente imprimir aqui o resultado na tela, a gente vê que ele é um dicionário que tem um monte de coisa. E um dos atributos desse dicionário é o texto da transcrição. Então, para a gente acessar especificamente o texto, a gente usa o result text. Ele está traduzindo como no IITAS, complicado de hoje. Você vai aprender o que são redeno-gares. Mas tem uma ideia de como as redeno-gares funcionam. Isso era para ser redes neurais. A gente pode fazer um novo teste utilizando o modelo um pouco melhor e ver se a qualidade da transcrição vai melhorar. Então, eu vou copiar esse mesmo código aqui e agora vou passar o small para ele. Eu vou sair do tiny, vou pular o base e vou para o small. Eu vou chamar de modelo 2 para eu não confundir de resultado 2. O vídeo que a gente quer passar é o mesmo. Então, agora a gente roda de novo e provavelmente vai demorar um pouco mais. Espera aí, na verdade, aqui não vai funcionar. Eu tenho que botar o modelo 2 aqui também. Agora sim. Enquanto a gente espera rodar, eu já vou preparar aqui a nossa próxima rodada, que vai ser um modelo 3 utilizando a versão média do modelo. O modelo terminou de rodar. Agora a gente pode ver o resultado 2. Ele saiu de redeno-gares para redes neurais. Ou seja, ele se aproximou um pouco mais do que seriam as redes neurais, mas ele ainda não chegou no resultado que a gente gostaria. A gente queria que ele escrevesse de fato redes neurais, mas você vê que ele consegue entender coisas aqui melhores. Então, aqui ele tinha entendido inteligência artificial como intergência especial. Aqui ele já transformou a inteligência artificial. Então, vamos ver se o modelo na versão medium consegue chegar na qualidade que a gente gostaria. De pelo menos identificar as duas palavras principais do vídeo que são redes neurais. O nosso modelo medium terminou de rodar e agora é expectativa para a gente ver o resultado. Veja que essa introdução aqui do vídeo está bem difícil para ele pegar, que seria no IA Descomplicada de hoje. Ele está transcrevendo como no IAres Complicada. Mas finalmente ele entendeu que o vídeo é sobre redes neurais. Então, entendeu ou não? Ele transcreveu o vídeo sobre redes neurais usando a palavra redes neurais. Então, aqui ele consegue agora dizer que você vai aprender o que são redes neurais, vai ter uma ideia de como as redes neurais funcionam e também de porque elas são tão poderosas. Porque nós utilizamos redes neurais para fazer aplicações. Ficou muito boa essa versão. Então, acho que agora a gente já tem uma versão inclusive legal para a gente salvar no arquivo de texto. Acho que essa é uma funcionalidade que faz sentido eu colocar aqui no vídeo porque se você está fazendo esse processo de transcrição para utilizar esse texto para alguma coisa, faz muito sentido que você queira tirar o texto aqui de dentro do notebook e levar para algum outro lugar, por meio de um arquivo. Para isso a gente vai utilizar esse pequeno trecho de código. Eu abro um novo arquivo com o nome e com o id do vídeo e o tamanho do modelo que eu utilizei, ponto txt. Aí aqui eu crio uma variável que vai ser esse arquivo de texto onde eu vou escrever. Então, a cada ponto, a cada sentença que aparece termina com ponto. Eu pego a sentença, escrevo no arquivo e pulo o linha lá no meu arquivo. E aí a gente pode rodar esse trecho de código e o arquivo txt também vai aparecer aqui na lateral. Então, a gente consegue abrir ele aqui dentro do ambiente do colab e agora sim de fato ler a transcrição inteira, que é bem legal e a gente também consegue baixar. Eu vou baixar aqui para a gente poder abrir dentro do computador essa transcrição. Ah, eu acabei de perceber que eu salvei a versão errado. Eu queria salvar a versão do modelo 3, mas eu salvei a versão do modelo 1 aqui. Eu tenho que mudar aqui para result3. E aí sim, agora provavelmente está na versão correta. Agora sim, então vou baixar novamente. Então, vê que é uma transcrição bem longa, né? Aliás, esse vídeo é um vídeo muito longo, é um vídeo de 18 minutos. Ele não demorou tanto, né? Para a gente ver o quanto as inteligências artificiais realmente avançaram, porque a gente agora tem acesso gratuito à transcrição de um vídeo longo. E eu não vou fazer aqui a versão large. Se você quiser testar aí no seu caso, testa e me conta depois. Se você conseguiu também notar um ganho de melhora na qualidade da transcrição utilizando a maior versão do modelo. Se você gostou desse vídeo, dessa nova linha editorial e há na prática aqui no canal, deixe um comentário utilizando a hashtag IA. Esse vídeo fica por aqui. Muito obrigada e até a próxima.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["model3 = whisper.load_model(\"medium\")\n","result3=model2.transcribe('ftl8cKnjeDM.mp3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aDAAxvNKsPNL","executionInfo":{"status":"ok","timestamp":1716489783352,"user_tz":180,"elapsed":597334,"user":{"displayName":"André Amorim","userId":"04859568826717067647"}},"outputId":"6da24e35-b5f8-455b-ffb5-465ca354f26d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████| 1.42G/1.42G [00:37<00:00, 40.4MiB/s]\n","/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]}]},{"cell_type":"code","source":["result3['text']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"id":"fc6xLRtpsT2M","executionInfo":{"status":"ok","timestamp":1716489976250,"user_tz":180,"elapsed":3,"user":{"displayName":"André Amorim","userId":"04859568826717067647"}},"outputId":"2bd88cec-7015-42a0-c178-971111ff8174"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' A inspiração para esse vídeo veio desse post do Guilherme Felici, que passou por mim no LinkedIn, e ele disse que ajudou uma amiga jornalista a transcrever um vídeo do YouTube, e aí compartilhou o código para que outras pessoas pudessem também utilizar. Oi gente, eu sou a Kiz e esse é o canal Programação Dinâmica. Aqui você encontra vídeos sobre ensino de programação, ciência de dados, inteligência artificial, discussões sobre tecnologias e seus impactos na sociedade. Se você acompanha o nosso canal e se interessa por inteligência artificial, você provavelmente já nos viu discutindo as diferentes aplicações de inteligência artificial e os impactos, as implicações éticas inclusive que elas têm. Essas discussões são muito relevantes, a gente vai continuar fazendo elas aqui no canal, mas essa linha editorial e a Ana Prática é para a gente pensar em aplicações. E a gente vai começar hoje aplicando inteligência artificial para fazer transcrição de vídeos no YouTube, e para isso a gente vai utilizar o Whisper, que é um sistema de reconhecimento de fala da OpenAI. Aqui no site da OpenAI eles explicam que o Whisper é um sistema de reconhecimento automático de fala treinado em 680 mil horas de dados supervisionados em diversos idiomas, então multilinguês e multitarefa coletados da web. E eles conseguiram com esse sistema mostrar que o uso de um conjunto de dados grande e diversificado leva uma maior robustez aos sotaques, as ruídos de fundo e a linguagem técnica, permitindo a transcrição em vários idiomas e a tradução desses idiomas para o inglês. Então olha que interessante, além de você conseguir pegar um vídeo no YouTube em língua portuguesa ou em Romeno, jogar no Whisper e fazer a transcrição, você também consegue jogar esse vídeo e pedir a versão em inglês dele, porque ele é capaz de fazer transcrição e tradução. Se você tem interesse em se aprofundar nesse modelo, você pode dar uma olhada nesse site, eu vou deixar o link aqui na descrição, mas como eu falei, o nosso vídeo aqui é sobre a aplicação. A inspiração para esse vídeo veio desse post do Guilherme Felici, que passou por mim no LinkedIn, e ele disse que ajudou uma amiga jornalista a transcrever um vídeo do YouTube, e aí compartilhou o código para que outras pessoas pudessem também utilizar. De fato, eu sei que essa é uma demanda muito grande de jornalistas, que fazem entrevistas, ou que de fato pegam vídeos no YouTube já prontos e precisam da transcrição. Eu já dei aula no MBE de jornalismo e percebi isso na prática, os meus alunos jornalistas tinham essa demanda mesmo. Para conseguir executar a transcrição do vídeo, a gente vai usar dois pacotes, o OpenAI Whisper, que é justamente para fazer a transcrição, e a gente também vai usar esse outro aqui, que é para a gente conseguir fazer o download do áudio de um vídeo do YouTube. E aí sim, passar esse áudio para o sistema de reconhecimento de fala. É importante destacar que para utilizar o Whisper por meio desse pacote, OpenAI Whisper, a gente não precisa pagar, é gratuito para fazer essa transcrição de vídeo. Isso, inclusive, torna ainda mais interessante ter a possibilidade de fazer isso, certo? Então, vamos lá, a gente vai começar instalando os pacotes aqui no nosso ambiente do Google Collab, para isso a gente utiliza essa exclamação e o comando PIP INSTAL, seguida do nome do pacote. Se você já programou em Python, você provavelmente está bem familiarizado com isso. Depois de instalar esses pacotes, o nosso próximo passo é escolher o vídeo que a gente quer fazer, a transcrição, e fazer um download do áudio desse vídeo, utilizando o pacote ali, que é o YT, atrás da DLP, a abreviação dele. O vídeo que eu vou escolher é o vídeo mais recente do canal, em que o Arsene explica o que são redes neurais, e eu preciso apenas conseguir pegar o link desse vídeo aqui do meu computador, para colocar ele dentro do meu código. Eu vou guardar essa URL desse vídeo do YouTube, numa variável que se chama vídeo URL. O próximo passo é a gente fazer o download do áudio desse vídeo, utilizando esse comando desse pacote YT DLP. Então, é a forma, essa é a sintasse, como esse pacote é utilizado, e a gente precisa apenas rodar esse comando. E veja que aqui no final a gente está passando para ele a URL do vídeo em si. Então, ele gerou o arquivo de áudio relacionado ao vídeo, aqui nessa sessão de arquivos. Então, se no seu notebook não estiver aparecendo, você tem que clicar nessa pastinha aqui, e aparece aqui ao lado. Então, uma vez que a gente já tem o vídeo, agora sim, a gente consegue usar o whisper para fazer a transcrição do vídeo. Para fazer a transcrição de vídeo, a gente agora sim vai usar o pacote whisper que a gente instalou. E você vai ver que é bem simples, a gente vai precisar de dois comandos especificamente. A gente vai carregar o modelo e fazer a transcrição a partir do modelo que a gente carregou. Para carregar o modelo, a gente precisa especificar qual modelo, qual versão do modelo. E a gente tem algumas opções, que é tiny, base, small, medium e enlarge. Que são as opções que variam de acordo com a qualidade da transcrição e com o tamanho do modelo e o processamento dele. Então, a gente vai fazer aqui gradualmente testes, desde o tiny, depois a gente vai testar o small e o medium para entender o resultado, e as qualidades e resultados e a diferença. Se, de fato, ele vai melhorando, embora demore um pouco mais para processar, se ele consegue melhorar na qualidade das transcrições. Basicamente é isso aqui, a gente usa o whisper.loadModel, passa o tipo do modelo. Aqui eu vou usar o tiny primeiro, para a gente testar o mais simples. A gente tem tiny, base, small, medium e large. O large é o maior modelo que tem a maior qualidade e o maior tempo de processamento. Em seguida, a gente tem que indicar para ele o que a gente quer que ele transcreva. E aí a gente vai passar o nome do arquivo aqui que a gente baixou. Então, eu vou copiar aqui. Ele provavelmente vai copiar o caminho inteiro do arquivo, mas a gente vai passar só o nome dele mesmo aqui. E a gente pode rodar o modelo. Se a gente imprimir aqui o resultado na tela, a gente vê que ele é um dicionário que tem um monte de coisa. E um dos atributos desse dicionário é o texto da transcrição. Então, para a gente acessar especificamente o texto, a gente usa o result text. Ele está traduzindo como no IITAS, complicado de hoje. Você vai aprender o que são redeno-gares. Mas tem uma ideia de como as redeno-gares funcionam. Isso era para ser redes neurais. A gente pode fazer um novo teste utilizando o modelo um pouco melhor e ver se a qualidade da transcrição vai melhorar. Então, eu vou copiar esse mesmo código aqui e agora vou passar o small para ele. Eu vou sair do tiny, vou pular o base e vou para o small. Eu vou chamar de modelo 2 para eu não confundir de resultado 2. O vídeo que a gente quer passar é o mesmo. Então, agora a gente roda de novo e provavelmente vai demorar um pouco mais. Espera aí, na verdade, aqui não vai funcionar. Eu tenho que botar o modelo 2 aqui também. Agora sim. Enquanto a gente espera rodar, eu já vou preparar aqui a nossa próxima rodada, que vai ser um modelo 3 utilizando a versão média do modelo. O modelo terminou de rodar. Agora a gente pode ver o resultado 2. Ele saiu de redeno-gares para redes neurais. Ou seja, ele se aproximou um pouco mais do que seriam as redes neurais, mas ele ainda não chegou no resultado que a gente gostaria. A gente queria que ele escrevesse de fato redes neurais, mas você vê que ele consegue entender coisas aqui melhores. Então, aqui ele tinha entendido inteligência artificial como intergência especial. Aqui ele já transformou a inteligência artificial. Então, vamos ver se o modelo na versão medium consegue chegar na qualidade que a gente gostaria. De pelo menos identificar as duas palavras principais do vídeo que são redes neurais. O nosso modelo medium terminou de rodar e agora é expectativa para a gente ver o resultado. Veja que essa introdução aqui do vídeo está bem difícil para ele pegar, que seria no IA Descomplicada de hoje. Ele está transcrevendo como no IAres Complicada. Mas finalmente ele entendeu que o vídeo é sobre redes neurais. Então, entendeu ou não? Ele transcreveu o vídeo sobre redes neurais usando a palavra redes neurais. Então, aqui ele consegue agora dizer que você vai aprender o que são redes neurais, vai ter uma ideia de como as redes neurais funcionam e também de porque elas são tão poderosas. Porque nós utilizamos redes neurais para fazer aplicações. Ficou muito boa essa versão. Então, acho que agora a gente já tem uma versão inclusive legal para a gente salvar no arquivo de texto. Acho que essa é uma funcionalidade que faz sentido eu colocar aqui no vídeo porque se você está fazendo esse processo de transcrição para utilizar esse texto para alguma coisa, faz muito sentido que você queira tirar o texto aqui de dentro do notebook e levar para algum outro lugar, por meio de um arquivo. Para isso a gente vai utilizar esse pequeno trecho de código. Eu abro um novo arquivo com o nome e com o id do vídeo e o tamanho do modelo que eu utilizei, ponto txt. Aí aqui eu crio uma variável que vai ser esse arquivo de texto onde eu vou escrever. Então, a cada ponto, a cada sentença que aparece termina com ponto. Eu pego a sentença, escrevo no arquivo e pulo o linha lá no meu arquivo. E aí a gente pode rodar esse trecho de código e o arquivo txt também vai aparecer aqui na lateral. Então, a gente consegue abrir ele aqui dentro do ambiente do colab e agora sim de fato ler a transcrição inteira, que é bem legal e a gente também consegue baixar. Eu vou baixar aqui para a gente poder abrir dentro do computador essa transcrição. Ah, eu acabei de perceber que eu salvei a versão errado. Eu queria salvar a versão do modelo 3, mas eu salvei a versão do modelo 1 aqui. Eu tenho que mudar aqui para result3. E aí sim, agora provavelmente está na versão correta. Agora sim, então vou baixar novamente. Então, vê que é uma transcrição bem longa, né? Aliás, esse vídeo é um vídeo muito longo, é um vídeo de 18 minutos. Ele não demorou tanto, né? Para a gente ver o quanto as inteligências artificiais realmente avançaram, porque a gente agora tem acesso gratuito à transcrição de um vídeo longo. E eu não vou fazer aqui a versão large. Se você quiser testar aí no seu caso, testa e me conta depois. Se você conseguiu também notar um ganho de melhora na qualidade da transcrição utilizando a maior versão do modelo. Se você gostou desse vídeo, dessa nova linha editorial e há na prática aqui no canal, deixe um comentário utilizando a hashtag IA. Esse vídeo fica por aqui. Muito obrigada e até a próxima.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":[],"metadata":{"id":"4qOyEqBTxnYd"},"execution_count":null,"outputs":[]}]}